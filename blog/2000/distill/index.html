<!DOCTYPE html> <html prefix="og: http://ogp.me/ns#"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Processing and acquisition traces in visual encoders: What does CLIP know about your camera? | Ramos &amp; Ramos </title> <meta name="author" content="You R. Name"> <meta name="description" content="a lot, apparently"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/ramos-ramos.png?d1acd009b7549a2e52dc4d4ddb1fe7dc"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ramos-ramos.github.io/blog/2000/distill/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:title" content="Processing and acquisition traces in visual encoders: What does CLIP know about your camera?"> <meta name="twitter:description" content="a lot, apparently"> <meta property="og:url" content="https://ramos-ramos.github.io/blog/2000/distill/"> <meta property="og:title" content="Processing and acquisition traces in visual encoders: What does CLIP know about your camera?"> <meta property="og:description" content="a lot, apparently"> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Processing and acquisition traces in visual encoders: What does CLIP know about your camera?",
            "description": "a lot, apparently",
            "published": "January 01, 2000",
            "authors": [
              
              {
                "author": "Ryan Ramos",
                "authorURL": "https://ramos-ramos.github.io/",
                "affiliations": [
                  {
                    "name": "The University of Osaka",
                    "url": ""
                  }
                ],
                "notation": "*"
              },
              
              {
                "author": "Vladan Stojnić",
                "authorURL": "https://stojnicv.xyz/",
                "affiliations": [
                  {
                    "name": "VRG, Czech Technical University in Prague",
                    "url": ""
                  }
                ],
                "notation": "*"
              },
              
              {
                "author": "Giorgos Kordopatis-Zilos",
                "authorURL": "https://gkordo.github.io/",
                "affiliations": [
                  {
                    "name": "VRG, Czech Technical University in Prague",
                    "url": ""
                  }
                ],
                "notation": ""
              },
              
              {
                "author": "Yuta Nakashima",
                "authorURL": "https://www.n-yuta.jp/",
                "affiliations": [
                  {
                    "name": "The University of Osaka",
                    "url": ""
                  }
                ],
                "notation": ""
              },
              
              {
                "author": "Noa Garcia",
                "authorURL": "https://www.noagarciad.com/",
                "affiliations": [
                  {
                    "name": "The University of Osaka",
                    "url": ""
                  }
                ],
                "notation": ""
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            },
            "notations": [
             
             {
               "symbol": "*",
               "description": "Equal contribution."
             }
             
           ]
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ramos &amp; Ramos </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">cvs </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/cv_patrick/">patrick's cv</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/cv_ryan/">ryan's cv</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">blogs </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/main_blog/">main blog</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/personal_blog/">personal blog</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/sports_graphics/">sports graphics</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/_pages/acknowledgements/">acknowledgements </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Processing and acquisition traces in visual encoders: What does CLIP know about your camera?</h1> <p>a lot, apparently</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#processing-and-acquisition-traces-can-be-recovered">Processing and acquisition traces can be recovered</a> </div> <ul> <li> <a href="#recovering-processing-labels">Recovering processing labels</a> </li> <li> <a href="#recovering-acquisition-labels">Recovering acquisition labels</a> </li> </ul> <div> <a href="#traces-can-interfere-with-downstream-tasks">Traces can interfere with downstream tasks</a> </div> <ul> <li> <a href="#processing-traces-affect-knn-classification">Processing traces affect kNN classification</a> </li> <li> <a href="#acquisition-traces-distract-in-image-retrieval">Acquisition traces distract in image retrieval</a> </li> </ul> <div> <a href="#discussion">Discussion</a> </div> </nav> </d-contents> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <p><span id="introduction" style="position: relative; top: -66px;"></span></p> <p>It turns out that this whole time, some visual encoders are sensitive to <strong>processing parameters</strong> (e.g. JPEG compression) and <strong>acquisition parameters</strong> (e.g. camera model). This information can be recovered from the image representations produced by these visual encoders, which in turn can overshadow the image’s semantic information and affect performance on downstream tasks. This blog post provides a brief overview of these findings, which are detailed further in our ICCV 2025 highlight paper <a href="https://arxiv.org/abs/2508.10637" rel="external nofollow noopener" target="_blank">Processing and acquisition traces in visual encoders: What does CLIP know about your camera?</a>.</p> <div class="figcaption">For a quick overview, check out this thread from our paper's co-first author!</div> <div class="l-body"> <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:giqwiwrhritawydh44zxmoge/app.bsky.feed.post/3lwo7xswiu22n" data-bluesky-cid="bafyreidhhhhcbfsenqu6pax3kvyojcxdx6ayhl5is3s3f3h76gwpmovera" data-bluesky-embed-color-mode="system"> <p lang="en">Have you ever asked yourself how much your favorite vision model knows about image capture parameters (e.g., the amount of JPEG compression, the camera model, etc.)? Furthermore, could these parameters influence its semantic recognition abilities?<br><br><a href="https://bsky.app/profile/did:plc:giqwiwrhritawydh44zxmoge/post/3lwo7xswiu22n?ref_src=embed" rel="external nofollow noopener" target="_blank">[image or embed]</a></p>— Vladan Stojnić (<a href="https://bsky.app/profile/did:plc:giqwiwrhritawydh44zxmoge?ref_src=embed" rel="external nofollow noopener" target="_blank">@stojnicv.xyz</a>) <a href="https://bsky.app/profile/did:plc:giqwiwrhritawydh44zxmoge/post/3lwo7xswiu22n?ref_src=embed" rel="external nofollow noopener" target="_blank">August 18, 2025 at 7:48 PM</a> </blockquote> <script async="" src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script> </div> <div class="l-page"> <iframe src="/assets/plotly/camera_bias/sim_dist.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="figcaption">If you take an ImageNet image and calculate its cosine similarity with all the other images, you might get distributions that look like these. The similarities with images of the same class will skew higher than those with different classes, but whether or not the images have the same JPEG compression as the query image can also affect the similarities.</div> <h2 id="processing-and-acquisition-traces-can-be-recovered">Processing and acquisition traces can be recovered</h2> <p>To show that vision encoders incorporate processing and acquisition traces into their embeddings (and determine which encoders do this the most), we train linear classifiers over embeddings extracted from a variety of vision encoders, with the encoders kept frozen. We categorize them as:</p> <ul> <li> <strong>Supervised (SUP)</strong>: Models trained in a supervised manner (e.g. with an image classification objective). We study several variants of ResNet<d-cite key="he2016deep"></d-cite>, ConvNeXt<d-cite key="liu2022convnet"></d-cite>, and ViT<d-cite key="dosovitskiy2020image"></d-cite>.</li> <li> <strong>Self-supervised learning (SSL)</strong>: Models trained in a self-supervised manner (e.g. contrastive learning). We study several variants of DINO<d-cite key="caron2021emerging"></d-cite>, DINOv2<d-cite key="oquab2024dinov2"></d-cite>, and MoCov3<d-cite key="chen2021empirical"></d-cite>.</li> <li> <strong>Contrastive visual-language (CVL)</strong>: Models trained for vision-language alignment. We study several variants of CLIP<d-cite key="radford2021learning"></d-cite>, OpenCLIP<d-cite key="cherti2023reproducible"></d-cite>, and SigLIP<d-cite key="zhai2023sigmoid"></d-cite><d-cite key="tschannen2025siglip"></d-cite>.</li> </ul> <p>The idea is that if you can predict traces from a frozen model’s embeddings at rates higher than random chance, then the model is definitely encoding relevant information. Across our experiments we tune learning rate and weight decay<d-footnote>Our hyperparameter tuning and training implementation is based on <a href="https://github.com/naver/trex/tree/master/transfer" rel="external nofollow noopener" target="_blank">https://github.com/naver/trex/tree/master/transfer</a></d-footnote>, but we use different datasets and processing steps depending on what we’re trying to predict. These are covered in the next subsections.</p> <h3 id="recovering-processing-labels">Recovering processing labels</h3> <p>To probe for processing traces, we can take a dataset like ImageNet<d-cite key="russakovsky15imagenet"></d-cite> or iNaturalist 2018<d-cite key="horn18inaturalist"></d-cite><d-cite key="inaturalist2018"></d-cite> and process them ourselves, creating our own image-label pairs. We choose the following parameters:</p> <div class="figcaption">The processing parameters we analyze.</div> <table> <thead> <tr> <th>parameter</th> <th style="text-align: right"># classes</th> <th>description</th> </tr> </thead> <tbody> <tr> <td>JPEG compression</td> <td style="text-align: right">6</td> <td>amount of JPEG compression</td> </tr> <tr> <td>sharpening</td> <td style="text-align: right">3</td> <td>amount of sharpening</td> </tr> <tr> <td>resizing</td> <td style="text-align: right">3</td> <td>amount of resizing</td> </tr> <tr> <td>interpolation</td> <td style="text-align: right">4</td> <td>type of interpolation during resize</td> </tr> </tbody> </table> <p>For the specific classes we study and their implementation, feel free to check out the supplementary material of our paper.</p> <p>In creating our pairs for a specific parameter, we seek to balance our data, so the specific class we process our image into is randomly sampled. For example, when we’re creating our data for probing JPEG compression, there is \(\frac{1}{6}\) chance that the image is compressed with quality 95 and chroma-subsampling 4:2:0, a \(\frac{1}{6}\) chance that the quality is 95 and chroma-subsampling is 4:4:4, so on and so forth.</p> <style>.original-container{display:grid;grid-template-columns:repeat(1,80px);grid-auto-rows:80px;gap:5px}.processed-container{display:grid;grid-auto-flow:column;grid-auto-columns:80px;gap:5px}.image-box{display:flex;flex-direction:column;align-items:center;font-size:14px;text-align:center}.image-box img{width:100%;height:80px;object-fit:cover;display:block}.image-box span{margin-top:4px}</style> <div class="wrapper" style="gap: 2em;"> <div> <div class="original-container"> <div class="image-box"> <span>source</span> <img src="/assets/img/camera_bias/query.jpg"> </div> </div> </div> <div> <div class="processed-container"> <div class="image-box"> <span>75, 4:2:0</span> <img src="/assets/img/camera_bias/quality=75_subsampling=2.jpg"> </div> <div class="image-box"> <span>75, 4:4:4</span> <img src="/assets/img/camera_bias/quality=75_subsampling=0.jpg"> </div> <div class="image-box"> <span>85, 4:2:0</span> <img src="/assets/img/camera_bias/quality=85_subsampling=2.jpg"> </div> <div class="image-box"> <span>85, 4:4:4</span> <img src="/assets/img/camera_bias/quality=85_subsampling=0.jpg"> </div> <div class="image-box"> <span>95, 4:2:0</span> <img src="/assets/img/camera_bias/quality=95_subsampling=2.jpg"> </div> <div class="image-box"> <span>95, 4:4:4</span> <img src="/assets/img/camera_bias/quality=95_subsampling=0.jpg"> </div> </div> </div> </div> <div class="figcaption">An example of our probing data creation process using the JPEG compression parameter. Each image is processed into one of these classes at equal chance (in this example, the numbers refer to the quality and chroma-subsampling of the compression).</div> <p>The results of our experiments are shown below:</p> <div class="l-page"> <iframe src="/assets/plotly/camera_bias/acc_processing_prediction.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="figcaption">Test accuracies on processing label prediction. Hover over each bar to see which visual encoder it belongs to. CVLs perform well above random performance, with supervised ConvNeXts also showing good performance. SSL models perform closer to random accuracy.</div> <p>First of all, it’s clear that some visual encoder are performing well above random accuracy, meaning that processing traces are being stored in their embeddings. We observe this most strongly in CVLs, which can reach 80% test accuracy on JPEG compression (random chance is 16.67%, which is more than 4\(\times\) lower!). We also see that supervised ConvNeXts perform well, which is surprising given that the only information available during training are ImageNet labels. Lastly, we note that SSL models tend to perform the weakest. These observations are a commonly reoccuring point throughout this work.</p> <h3 id="recovering-acquisition-labels">Recovering acquisition labels</h3> <p>To probe for acquisition traces, we need a dataset that comes with relevant annotations. Fortunately, this can be extracted from images’ Exif metadata. Unfortunately, many readily available datasets don’t provide this metadata. Thus, we use the Flickr API<d-footnote><a href="https://www.flickr.com/services/api/" rel="external nofollow noopener" target="_blank">https://www.flickr.com/services/api/</a></d-footnote> to search for safe-for-work, permissively licensed photos and extract their Exif metadata to create our image-label pairs. We present this dataset, named FlickrExif, below <span style="color: red;">NOTE: FlickrExif is still private; ILIAS is shown below as a placeholder</span>:</p> <div class="l-page"> <iframe src="https://huggingface.co/datasets/vrg-prague/ilias/embed/viewer/core_db/core_db" frameborder="0" width="100%" height="560px"></iframe> </div> <p>With FlickrExif, we can study the following acquisition attributes:</p> <div class="figcaption">The acquisition parameters we analyze.</div> <table> <thead> <tr> <th>parameter</th> <th style="text-align: right"># classes</th> <th>description</th> </tr> </thead> <tbody> <tr> <td>make</td> <td style="text-align: right">9</td> <td>manufacturer of the camera</td> </tr> <tr> <td>model (all)</td> <td style="text-align: right">88</td> <td>specific camera model used</td> </tr> <tr> <td>model (smart)</td> <td style="text-align: right">12</td> <td>specific smartphone used</td> </tr> <tr> <td>model (smart vs non-smart)</td> <td style="text-align: right">2</td> <td>whether camera is a smartphone</td> </tr> <tr> <td>exposure</td> <td style="text-align: right">16</td> <td>amount of light captured by sensor</td> </tr> <tr> <td>aperture</td> <td style="text-align: right">17</td> <td>size of the opening in the lens</td> </tr> <tr> <td>ISO speed</td> <td style="text-align: right">16</td> <td>camera sensor’s sensitivity to light</td> </tr> <tr> <td>focal length</td> <td style="text-align: right">13</td> <td>distance from lens to sensor</td> </tr> </tbody> </table> <p>Now, there’s a possibility that there are correlations between image semantics and acquisition labels. For example, if nighttime photos are commonly shot with higher ISO values, then instead of determining whether encoders are sensitive to the low-level features associated with different ISO values, we might simply end up determining which ones can distinguish between daytime and nighttime photos.</p> <p>To avoid this, we have two fixes. The first is to limit the amount of photos a Flickr user contributes per month-year in our dataset (if wedding photographer Bob took 1,000 photos in one day with similar camera settings, we’d be in trouble). The second is to simply mask a large portion of the image (90%) to scrub out the semantic information. The first fix is incorporated into our dataset, while we leave the second as the recommend preprocessing procedure for future users.</p> <p>With that established, our results are below:</p> <div class="l-page"> <iframe src="/assets/plotly/camera_bias/acquisition_plots_ninety.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="figcaption">Test accuracies on acquisition label prediction. Hover over each bar to see which visual encoder it belongs to. Similar to the last experiment, we observe strong performance from CVLs and supervised ConvNeXts and near-random performance from SSL models.</div> <p>We observe a very similar pattern in these results. CVLs, with the exception of SigLIPs, perform very well, alongside supervised ConvNeXts. Meanwhile, SSL models seem to encode very little acquisition information.</p> <h2 id="traces-can-interfere-with-downstream-tasks">Traces can interfere with downstream tasks</h2> <p>Now that we’ve (quite clearly) established that some visual encoders do indeed encode processing and acquisition traces, we want to show why you should care. We show an example for each type of trace: kNN classification and image retrieval.</p> <h3 id="processing-traces-affect-knn-classification">Processing traces affect kNN classification</h3> <p>Consider four different types of training sets for a kNN classifier:</p> <ul> <li> <strong>All-same (baseline)</strong>: all train and test images are processed identically</li> <li> <strong>All-diff</strong>: test images are processed differently from train images</li> <li> <strong>Pos-same</strong>: train and test images only have the same processing attributes if they share the same class, and are different otherwise</li> <li> <strong>Neg-same</strong>: train and test images only have the same processing attributes if they do not share the same class, and are identical otherwise</li> <li> <strong>Uniform</strong>: train images are processed one way while the test images are processed randomly in a uniform manner</li> </ul> <p>We plot the results for each set-up below:</p> <div class="l-page"> <iframe src="/assets/plotly/camera_bias/processing_influence.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="figcaption">Accuracies across the different set-ups. For some models, whether the query image's metadata aligns with the positives or negatives significantly impacts performance. For some, there is very little effect (try zooming in on the SSL models). ImageNet classification uses k=10 while iNaturalist classification uses k=1.</div> <p>If you look at the CVLs, we can see that performance tends to wildly differ depending on whether it’s the positivies or the negatives that match the query’s metadata. In the latter case, the metadata becomes the dominant factor in determining the nearest neighbors, essentially “distracting” the classifier and tanking its performance. Conversely, we see that processing has minimal effect in the case of SSL models, showcasing the lack of potentially distracting processing traces in their embeddings.</p> <p>Take a look below for a direct visualization of how the top-\(k\) neighbors identified by a CVL ConvNeXt-L are affected by JPEG compression. Making the positives’ metadata match the query’s pushes them closer towards the query, while doing so with the negatives pushes the negatives closer towards the query.</p> <style>.topk-query-icon{opacity:1;cursor:default}.topk-grid{display:grid;grid-template-columns:auto auto auto auto 1fr;grid-auto-rows:auto;gap:.75rem 1.5rem;align-items:center}.topk-cell{opacity:.35;cursor:pointer;transition:opacity .2s;line-height:0}.topk-cell.active{opacity:1}.topk-icon{width:40px;height:auto}.topk-query-large img,.topk-gallery img{height:60px;width:auto;object-fit:contain;border:3px solid gray;border-radius:2px}.topk-gallery img{border:3px solid transparent}.topk-h-query{grid-column:1;grid-row:1}.topk-h-pos{grid-column:2;grid-row:1}.topk-h-neg{grid-column:3;grid-row:1}.topk-pos-r1{grid-column:2;grid-row:2}.topk-neg-r1{grid-column:3;grid-row:2}.topk-query-mid{grid-column:1;grid-row:3}.topk-pos-r2{grid-column:2;grid-row:3}.topk-neg-r2{grid-column:3;grid-row:3}.topk-query-large{grid-column:4;grid-row:3}#topk-gallery{grid-column:5;grid-row:3;display:flex;gap:.5rem}.topk-pos-r3{grid-column:2;grid-row:4}.topk-neg-r3{grid-column:3;grid-row:4}</style> <div class="l-page" style="display: flex; justify-content: center;"> <div class="topk-grid"> <div class="topk-h-query">query</div> <div class="topk-h-pos">positives</div> <div class="topk-h-neg">negatives</div> <div></div> <div></div> <div class="topk-cell topk-pos-r1" data-row="1" data-folder="/assets/img/camera_bias/baseline"> <img class="topk-icon" src="/assets/img/camera_bias/p_75.png" alt="p_75"> </div> <div class="topk-cell topk-neg-r1" data-row="1" data-folder="/assets/img/camera_bias/baseline"> <img class="topk-icon" src="/assets/img/camera_bias/n_75.png" alt="n_75"> </div> <div class="topk-query-mid"> <img class="topk-icon topk-query-icon" src="/assets/img/camera_bias/q_75.png" alt="q_75"> </div> <div class="topk-cell topk-pos-r2" data-row="2" data-folder="/assets/img/camera_bias/pos"> <img class="topk-icon" src="/assets/img/camera_bias/p_75.png" alt="p_75"> </div> <div class="topk-cell topk-neg-r2" data-row="2" data-folder="/assets/img/camera_bias/pos"> <img class="topk-icon" src="/assets/img/camera_bias/n_95.png" alt="n_95"> </div> <div class="topk-query-large"> <img src="/assets/img/camera_bias/query.jpg" alt="query"> </div> <div class="topk-gallery" id="topk-gallery"> <img><img><img><img><img><img><img><img><img><img> </div> <div class="topk-cell topk-pos-r3" data-row="3" data-folder="/assets/img/camera_bias/neg"> <img class="topk-icon" src="/assets/img/camera_bias/p_95.png" alt="p_95"> </div> <div class="topk-cell topk-neg-r3" data-row="3" data-folder="/assets/img/camera_bias/neg"> <img class="topk-icon" src="/assets/img/camera_bias/n_75.png" alt="n_75"> </div> </div> </div> <script>function setActiveRow(e){cells.forEach(r=>r.classList.toggle("active",r.dataset.row===String(e)))}function loadGallery(e){const r=galleryColors[e]||[];gallery.querySelectorAll("img").forEach((a,l)=>{a.classList.remove("loaded"),a.src=`${e}/${l}.jpg`,a.style.border=`3px solid ${r[l]||"transparent"}`,a.onload=()=>a.classList.add("loaded")})}const gallery=document.getElementById("topk-gallery"),cells=document.querySelectorAll(".topk-cell"),galleryColors={"/assets/img/camera_bias/baseline":["green","green","red","green","red","green","red","red","red","red"],"/assets/img/camera_bias/pos":["green","green","green","green","green","green","green","green","red","red"],"/assets/img/camera_bias/neg":["red","green","green","red","red","red","red","red","red","red"]};cells.forEach(e=>{e.addEventListener("click",()=>{const r=e.dataset.row,a=e.dataset.folder;setActiveRow(r),loadGallery(a)})}),setActiveRow(1),loadGallery("/assets/img/camera_bias/baseline");</script> <div class="figcaption">The top 10 neighbors retrieved using the embeddings from a CVL ConvNeXt-L model. JPEG compression can influence which images are the closest, even pushing them over images with semantics more aligned to the query. The query image is denoted with a gray border. Retrieved images of the same semantic class are denoted with green borders, while those of different semantic classes are shown with red borders.</div> <h3 id="acquisition-traces-distract-in-image-retrieval">Acquisition traces distract in image retrieval</h3> <p>Imagine an image retrieval scenario, where the target image was captured with a type of camera different from the one used to capture the query image. Now imagine two possible collections:</p> <ul> <li> <strong>same</strong>: one where the rest of the collection were also captured by a different camera</li> <li> <strong>different</strong>: one where the non-target collection images were captured with the same camera as the query</li> </ul> <p>Similar to the previous subsection, we show that acquisition traces, in this case camera type, can also dominate over and distract from semantic information, affecting retrieval.</p> <p>To do this, we collect a dataset we dub PairCams, available <a href="https://huggingface.co/datasets/ryanramos/PairCams" rel="external nofollow noopener" target="_blank">here</a>. We capture 730 pairs of photos of the <em>exact same subject</em> with nearly <em>identical shooting conditions</em> (e.g. angle, camera orientation, time of day, camera shooting mode), with the only difference being camera type. We experiment with modern smartphones and older digital cameras. Despite the difference in camera, the image pairs contain nearly identical semantic content, meaning that it should be trivial to retrieve one using the other.</p> <style>.wrapper{display:flex;gap:5rem;justify-content:center;flex-wrap:wrap}.grid-container{display:grid;grid-template-columns:repeat(2,150px);grid-auto-rows:150px;gap:5px}.grid-container img{width:100%;height:100%;object-fit:cover;transition:opacity .3s;display:block}.dimmed{opacity:.3;.grid-title{text-align:center;margin-bottom:.5rem;font-weight:bold;font-size:1rem}}</style> <div class="wrapper"> <div> <div class="grid-title">modern smartphones</div> <div class="grid-container" id="left-grid"> <img src="/assets/img/camera_bias/b_0.jpg" data-index="0"> <img src="/assets/img/camera_bias/b_1.jpg" data-index="1"> <img src="/assets/img/camera_bias/b_2.jpg" data-index="2"> <img src="/assets/img/camera_bias/b_3.jpg" data-index="3"> <img src="/assets/img/camera_bias/b_4.jpg" data-index="4"> <img src="/assets/img/camera_bias/b_5.jpg" data-index="5"> <img src="/assets/img/camera_bias/b_6.jpg" data-index="6"> <img src="/assets/img/camera_bias/b_7.jpg" data-index="7"> </div> </div> <div> <div class="grid-title">older digital cameras</div> <div class="grid-container" id="right-grid"> <img src="/assets/img/camera_bias/a_0.jpg" data-index="0"> <img src="/assets/img/camera_bias/a_1.jpg" data-index="1"> <img src="/assets/img/camera_bias/a_2.jpg" data-index="2"> <img src="/assets/img/camera_bias/a_3.jpg" data-index="3"> <img src="/assets/img/camera_bias/a_4.jpg" data-index="4"> <img src="/assets/img/camera_bias/a_5.jpg" data-index="5"> <img src="/assets/img/camera_bias/a_6.jpg" data-index="6"> <img src="/assets/img/camera_bias/a_7.jpg" data-index="7"> </div> </div> </div> <div class="figcaption">Examples from PairCams. Each image is captured twice, once with a modern smartphone and once with an older digital camera.</div> <script>function setupHoverSync(e,t){function d(e,t,d){if("IMG"!==e.target.tagName)return;const r=e.target.dataset.index;t.querySelectorAll("img").forEach(e=>{e.dataset.index!==r&&e.classList.add("dimmed")}),d.querySelectorAll("img").forEach(e=>{e.dataset.index!==r&&e.classList.add("dimmed")})}function r(e,t,d){t.querySelectorAll("img").forEach(e=>e.classList.remove("dimmed")),d.querySelectorAll("img").forEach(e=>e.classList.remove("dimmed"))}e.addEventListener("mouseover",r=>d(r,e,t)),t.addEventListener("mouseover",r=>d(r,t,e)),e.addEventListener("mouseout",d=>r(d,e,t)),t.addEventListener("mouseout",d=>r(d,t,e))}setupHoverSync(document.getElementById("left-grid"),document.getElementById("right-grid"));</script> <p>We go through every visual encoder we’ve used so far to calculate the recall@\(1\) for each collection:</p> <div class="l-page"> <iframe src="/assets/plotly/camera_bias/ndidscatter.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="figcaption">Recall@1 on the <i>same</i> and <i>different</i> collections. Zoom into the square for a closer look. Again, CVL models show the most sensitivity, with SSL models showing the least. A plot on y=x implies completely robust performance.</div> <p>The \(y=x\) line shows where a perfectly robust visual encoder should lie. If a visual encoder is not prone to being distracted by acquisition parameters as it searches for the best semantic match, then performance should be equal in both settings. What we observe however is that CVLs again show an extreme sensitivity to acquisition parameters. A visual encoder with a near perfect recall@\(1\) with the <em>different</em> collection can drop to 0.85 recall@\(1\) in the <em>same</em> setting. Zooming into the black square shows another consistent finding: SSL models are among the most robust, staying much closer to the \(y=x\) line than any other model.</p> <h2 id="discussion">Discussion</h2> <p>The evidence that some visual encoders also encode processing and acquisition traces is strong, but we’re still in the middle of figuring out the possibly <em>why’s</em> behind all of this. It’s a little difficult to test our theories given the use of private datasets for existing models, and the cost of training our own models from scratch. We also have yet to think of mitigation techniques.</p> <p>This phenomenon we observe of course carries implications. Firstly, the fact that metadata traces can overshadow semantic information raises concerns over the robustness and trustworthiness of our current algorithms. We wouldn’t want a malicious agent messing with a model simply by playing with images’ metadata, especially in critical domains like healthcare or autonomous systems. However, there is also the implication that this information being available through off-the-shelf models can help with digital forensics research or deepfake detection.</p> <p>Thank you for reading!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/camera_bias.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Ramos-Ramos/ramos-ramos.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Patrick Ramos and Ryan Ramos. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>