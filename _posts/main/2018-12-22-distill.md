---
layout: distill
title: "Processing and acquisition traces in visual encoders: What does CLIP know about your camera?"
description: a lot, apparently
tags: distill formatting
giscus_comments: true
date: 2000-01-01
featured: true
categories: main

authors:
  - name: Ryan Ramos
    url: "https://ramos-ramos.github.io/"
    affiliations:
      name: The University of Osaka
    notation: "*"
  - name: Vladan Stojnić
    url: "https://stojnicv.xyz/"
    affiliations:
      name: VRG, Czech Technical University in Prague
    notation: "*"
  - name: Giorgos Kordopatis-Zilos
    url: "https://gkordo.github.io/"
    affiliations:
      name: VRG, Czech Technical University in Prague
  - name: Yuta Nakashima
    url: "https://www.n-yuta.jp/"
    affiliations:
      name: The University of Osaka
  - name: Noa Garcia
    url: "https://www.noagarciad.com/"
    affiliations:
      name: The University of Osaka

notations:
  - symbol: "*"
    description: Equal contribution.

bibliography: camera_bias.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction
  - name: Processing and acquisition traces can be recovered
    subsections:
      - name: Recovering processing labels
      - name: Recovering acquisition labels 
  - name: Traces can interfere with downstream tasks
    subsections:
      - name: Processing traces affect nearest neighbors
      - name: Acquisition traces distract in image retrieval
  - name: Equations
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Citations
  - name: Footnotes
  - name: Code Blocks
  - name: Interactive Plots
  - name: Layouts
  - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<span id="introduction" style="position: relative; top: -66px;"></span>

<!-- Visual encoders are everywhere because image representations are the starting point for many vision tasks. Because of this ubiquity, multiple works have probed the sensitivity of these encoders to distribution shifts such as corruptions or stylistic changes. In our paper [Processing and acquisition traces in visual encoders: What does CLIP know about your camera?](https://arxiv.org/abs/2508.10637), we show that visual encoders are also sensitive to **processing parameters** (e.g. JPEG compression) and **acquisition parameters** (e.g. camera model). This information can be recovered from image representations depending on the visual encoder, and this information can overshadow the image's semantic information and affect performance on downstream tasks. -->

It turns out that this whole time, some visual encoders are sensitive to **processing parameters** (e.g. JPEG compression) and **acquisition parameters** (e.g. camera model). This information can be recovered from the image representations produced by these visual encoders, which in turn can overshadow the image's semantic information and affect performance on downstream tasks. This blog post provides a brief overview of these findings, which are detailed further in our ICCV 2025 highlight paper [Processing and acquisition traces in visual encoders: What does CLIP know about your camera?](https://arxiv.org/abs/2508.10637).

<div class="l-body">
  <iframe src="{{ '/assets/plotly/camera_bias/sim_dist.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
<div class="figcaption">If you take an ImageNet image and calculate its cosine similarity with all the other images, you might get distributions that look like these. The similarities with images of the same class will skew higher than those with different classes, but whether or not the images have the same JPEG quality as the query image can also affect the similarities.</div>


## Processing and acquisition traces can be recovered

To show that vision encoders incorporate processing and acquisition traces into their embeddings (and determine which encoders do this the most), we train linear classifiers over embeddings extracted from a variety of vision encoders, with the encoders kept frozen. We categorize them as:

- **Supervised (SUP)**: Models trained in a supervised manner (e.g. with an image classification objective). We study several variants of ResNet<d-cite key="he2016deep"></d-cite>, ConvNeXt<d-cite key="liu2022convnet"></d-cite>, and ViT<d-cite key="dosovitskiy2020image"></d-cite>.
- **Self-supervised learning (SSL)**: Models trained in a self-supervised manner (e.g. contrastive learning). We study several variants of DINO<d-cite key="caron2021emerging"></d-cite>, DINOv2<d-cite key="oquab2024dinov2"></d-cite>, and MoCov3<d-cite key="chen2021empirical"></d-cite>.
- **Contrastive visual-language (CVL)**: Models trained for vision-language alignment. We study several variants of CLIP<d-cite key="radford2021learning"></d-cite>, OpenCLIP<d-cite key="cherti2023reproducible"></d-cite>, and SigLIP<d-cite key="zhai2023sigmoid"></d-cite><d-cite key="tschannen2025siglip"></d-cite>.

The idea is that if you can predict traces from a frozen model's embeddings at rates higher than random chance, then the model is definitely encoding relevant information. Across our experiments we tune learning rate and weight decay, but we use different datasets and processing steps depending on what we're trying to predict. These are covered in the next subsections.

### Recovering processing labels

To probe for processing traces, we can take a dataset like ImageNet<d-cite key="russakovsky15imagenet"></d-cite> or iNaturalist 2018<d-cite key="horn18inaturalist"></d-cite><d-cite key="inaturalist2018"></d-cite> and process them ourselves, creating our own image-label pairs. We choose the following parameters:

<div class="figcaption">The processing parameters we analyze.</div>

| parameter        | # classes | description                         |
| ---------------- | --------: | ----------------------------------- |
| JPEG compression | 6         | amount of JPEG compression          |
| sharpening       | 3         | amount of sharpening                |
| resizing         | 3         | amount of resizing                  |
| interpolation    | 4         | type of interpolation during resize |

For the specific classes we study and their implementation, feel free to check out the supplementary material of our paper.

In creating our pairs for a specific parameter, we seek to balance our data, so the specific class we process our image into is randomly sampled. For example, when we're creating our data for probing JPEG compression, there is $$ \frac{1}{6} $$ chance that the image is compressed with quality 95 and chroma-subsampling 4:2:0, a $$ \frac{1}{6} $$ chance that the quality is 95 and chroma-subsampling is 4:4:4, so on and so forth.

<style>
.original-container {
  display: grid;
  grid-template-columns: repeat(1, 80px);
  grid-auto-rows: 80px;                  
  gap: 5px;
}

.processed-container {
  display: grid;
  grid-auto-flow: column;
  grid-auto-columns: 80px;               
  gap: 5px;
}

.image-box {
  display: flex;
  flex-direction: column;
  align-items: center;
  font-size: 14px;
  text-align: center;
}

.image-box img {
  width: 100%;
  height: 80px;
  object-fit: cover;
  display: block;
}

.image-box span {
  margin-top: 4px;
}
</style>

<div class="wrapper" style="gap: 2em;">
  <div>
    <div class="original-container">
      <div class="image-box">
        <span>source</span>
        <img src="/assets/img/camera_bias/query.jpg">
      </div>
    </div>
  </div>

  <div>
    <div class="processed-container">
      <div class="image-box">
        <span>75, 4:2:0</span>
        <img src="/assets/img/camera_bias/quality=75_subsampling=2.jpg">
      </div>
      <div class="image-box">
        <span>75, 4:4:4</span>
        <img src="/assets/img/camera_bias/quality=75_subsampling=0.jpg">
      </div>
      <div class="image-box">
        <span>85, 4:2:0</span>
        <img src="/assets/img/camera_bias/quality=85_subsampling=2.jpg">
      </div>
      <div class="image-box">
        <span>85, 4:4:4</span>
        <img src="/assets/img/camera_bias/quality=85_subsampling=0.jpg">
      </div>
      <div class="image-box">
        <span>95, 4:2:0</span>
        <img src="/assets/img/camera_bias/quality=95_subsampling=2.jpg">
      </div>
      <div class="image-box">
        <span>95, 4:4:4</span>
        <img src="/assets/img/camera_bias/quality=95_subsampling=0.jpg">
      </div>
    </div>
  </div>
</div>
<div class="figcaption">An example of our probing data creation process using the JPEG compression parameter. Each image is processed into one of these classes at equal chance (in this example, the numbers refer to the quality and chroma-subsampling of the compression).</div>

<br>

The results of our experiments are shown below:

<div class="l-page">
  <iframe src="{{ '/assets/plotly/camera_bias/acc_processing_prediction.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
<div class="figcaption">Test accuracies on processing label prediction. Hover over each bar to see which visual encoder it belongs to.</div>

First of all, it's clear that some visual encoder are performing well above random accuracy, meaning that processing traces are being stored in their embeddings. We observe this most strongly in CVLs, which can reach 80% test accuracy on JPEG compression (random chance is 16.67%, which is more than 4$$ \times $$ lower!). We also see that supervised ConvNeXts perform well, which is surprising given that the only information available during training are ImageNet labels. Lastly, we note that SSL models tend to perform the weakest. These observations are a commonly reoccuring point throughout this work.


### Recovering acquisition labels

<div class="l-page">
  <iframe src="{{ '/assets/plotly/camera_bias/acquisition_plots_ninety.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
<div class="figcaption">Insert caption here.</div>


## Traces can interfere with downstream tasks


### Processing traces affect nearest neighbors


### Acquisition traces distract in image retrieval

<style>
.wrapper {
  display: flex;
  gap: 5rem;       /* space between left and right grids */
  justify-content: center;
  flex-wrap: wrap; /* allows stacking on narrow screens */
}

.grid-container {
  display: grid;
  grid-template-columns: repeat(2, 150px); /* 2 columns */
  grid-auto-rows: 150px;                   /* 4 rows (automatically) */
  gap: 5px;
}

.grid-container img {
  width: 100%;
  height: 100%;
  object-fit: cover;   /* handle different image sizes */
  transition: opacity 0.3s;
  display: block;
}

.dimmed {
  opacity: 0.3;

.grid-title {
  text-align: center;     /* center the text above the grid */
  margin-bottom: 0.5rem;  /* space between title and grid */
  font-weight: bold;      /* optional, make it bold */
  font-size: 1rem;        /* adjust size as needed */
}
}
</style>

<div class="wrapper">
  <!-- Left grid -->
  <div>
    <div class="grid-title">modern smartphones</div>
    <div class="grid-container" id="left-grid">
      <img src="/assets/img/camera_bias/b_0.jpg" data-index="0">
      <img src="/assets/img/camera_bias/b_1.jpg" data-index="1">
      <img src="/assets/img/camera_bias/b_2.jpg" data-index="2">
      <img src="/assets/img/camera_bias/b_3.jpg" data-index="3">
      <img src="/assets/img/camera_bias/b_4.jpg" data-index="4">
      <img src="/assets/img/camera_bias/b_5.jpg" data-index="5">
      <img src="/assets/img/camera_bias/b_6.jpg" data-index="6">
      <img src="/assets/img/camera_bias/b_7.jpg" data-index="7">
    </div>
  </div>

  <!-- Right grid -->
  <div>
    <div class="grid-title">older digital cameras</div>
    <div class="grid-container" id="right-grid">
      <img src="/assets/img/camera_bias/a_0.jpg" data-index="0">
      <img src="/assets/img/camera_bias/a_1.jpg" data-index="1">
      <img src="/assets/img/camera_bias/a_2.jpg" data-index="2">
      <img src="/assets/img/camera_bias/a_3.jpg" data-index="3">
      <img src="/assets/img/camera_bias/a_4.jpg" data-index="4">
      <img src="/assets/img/camera_bias/a_5.jpg" data-index="5">
      <img src="/assets/img/camera_bias/a_6.jpg" data-index="6">
      <img src="/assets/img/camera_bias/a_7.jpg" data-index="7">
    </div>
  </div>
</div>
<div class="figcaption">Insert caption here.</div>

<script>
function setupHoverSync(leftGrid, rightGrid) {
  function handleHover(e, sourceGrid, targetGrid) {
    if (e.target.tagName !== "IMG") return;
    const idx = e.target.dataset.index;

    sourceGrid.querySelectorAll("img").forEach(img => {
      if (img.dataset.index !== idx) img.classList.add("dimmed");
    });
    targetGrid.querySelectorAll("img").forEach(img => {
      if (img.dataset.index !== idx) img.classList.add("dimmed");
    });
  }

  function reset(e, sourceGrid, targetGrid) {
    sourceGrid.querySelectorAll("img").forEach(img => img.classList.remove("dimmed"));
    targetGrid.querySelectorAll("img").forEach(img => img.classList.remove("dimmed"));
  }

  leftGrid.addEventListener("mouseover", e => handleHover(e, leftGrid, rightGrid));
  rightGrid.addEventListener("mouseover", e => handleHover(e, rightGrid, leftGrid));
  leftGrid.addEventListener("mouseout", e => reset(e, leftGrid, rightGrid));
  rightGrid.addEventListener("mouseout", e => reset(e, rightGrid, leftGrid));
}

setupHoverSync(
  document.getElementById("left-grid"),
  document.getElementById("right-grid")
);
</script>

<div class="l-page">
  <iframe src="{{ '/assets/plotly/camera_bias/ndidscatter.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
<div class="figcaption">Insert caption here.</div>


## Equations

This theme supports rendering beautiful math in inline and display modes using [MathJax 3](https://www.mathjax.org/) engine.
You just need to surround your math expression with `$$`, like `$$ E = mc^2 $$`.
If you leave it inside a paragraph, it will produce an inline expression, just like $$ E = mc^2 $$.

To use display mode, again surround your expression with `$$` and place it as a separate paragraph.
Here is an example:

$$
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
$$

Note that MathJax 3 is [a major re-write of MathJax](https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html) that brought a significant improvement to the loading and rendering speed, which is now [on par with KaTeX](http://www.intmath.com/cg5/katex-mathjax-comparison.php).

---

## Citations

Citations are then used in the article body with the `<d-cite>` tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.

<!-- The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). -->
If you have an appendix, a bibliography is automatically created and populated in it.

Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.

---

## Footnotes

Just wrap the text you would like to show up in a footnote in a `<d-footnote>` tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>

---

## Code Blocks

Syntax highlighting is provided within `<d-code>` tags.
An example of inline code snippets: `<d-code language="html">let x = 10;</d-code>`.
For larger blocks of code, add a `block` attribute:

<d-code block language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

**Note:** `<d-code>` blocks do not look good in the dark mode.
You can always use the default code-highlight using the `highlight` liquid tag:

{% highlight javascript %}
var x = 25;
function(x) {
return x \* x;
}
{% endhighlight %}

---

## Interactive Plots

You can add interative plots using plotly + iframes :framed_picture:

<div class="l-page">
  <iframe src="{{ '/assets/plotly/demo.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:

{% highlight python %}
import pandas as pd
import plotly.express as px
df = pd.read_csv(
'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv'
)
fig = px.density_mapbox(
df,
lat='Latitude',
lon='Longitude',
z='Magnitude',
radius=10,
center=dict(lat=0, lon=180),
zoom=0,
mapbox_style="stamen-terrain",
)
fig.show()
fig.write_html('assets/plotly/demo.html')
{% endhighlight %}

---

## Details boxes

Details boxes are collapsible boxes which hide additional information from the user. They can be added with the `details` liquid tag:

{% details Click here to know more %}
Additional details, where math $$ 2x - 1 $$ and `code` is rendered correctly.
{% enddetails %}

---

## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

---

## Other Typography?

Emphasis, aka italics, with _asterisks_ (`*asterisks*`) or _underscores_ (`_underscores_`).

Strong emphasis, aka bold, with **asterisks** or **underscores**.

Combined emphasis with **asterisks and _underscores_**.

Strikethrough uses two tildes. ~~Scratch this.~~

1. First ordered list item
2. Another item
   ⋅⋅\* Unordered sub-list.
3. Actual numbers don't matter, just that it's a number
   ⋅⋅1. Ordered sub-list
4. And another item.

⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).

⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)

- Unordered list can use asterisks

* Or minuses

- Or pluses

[I'm an inline-style link](https://www.google.com)

[I'm an inline-style link with title](https://www.google.com "Google's Homepage")

[I'm a reference-style link][Arbitrary case-insensitive reference text]

[You can use numbers for reference-style link definitions][1]

Or leave it empty and use the [link text itself].

URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <http://www.example.com> and sometimes
example.com (but not on Github, for example).

Some text to show that the reference links can follow later.

[arbitrary case-insensitive reference text]: https://www.mozilla.org
[1]: http://slashdot.org
[link text itself]: http://www.reddit.com

Here's our logo (hover to see the title text):

Inline-style:
![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 1")

Reference-style:
![alt text][logo]

[logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 2"

Inline `code` has `back-ticks around` it.

```javascript
var s = "JavaScript syntax highlighting";
alert(s);
```

```python
s = "Python syntax highlighting"
print s
```

```
No language indicated, so no syntax highlighting.
But let's throw in a <b>tag</b>.
```

Colons can be used to align columns.

| Tables        |      Are      |  Cool |
| ------------- | :-----------: | ----: |
| col 3 is      | right-aligned | $1600 |
| col 2 is      |   centered    |   $12 |
| zebra stripes |   are neat    |    $1 |

There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don't need to make the
raw Markdown line up prettily. You can also use inline Markdown.

| Markdown | Less      | Pretty     |
| -------- | --------- | ---------- |
| _Still_  | `renders` | **nicely** |
| 1        | 2         | 3          |

> Blockquotes are very handy in email to emulate reply text.
> This line is part of the same quote.

Quote break.

> This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can _put_ **Markdown** into a blockquote.

Here's a line for us to start with.

This line is separated from the one above by two newlines, so it will be a _separate paragraph_.

This line is also a separate paragraph, but...
This line is only separated by a single newline, so it's a separate line in the _same paragraph_.
