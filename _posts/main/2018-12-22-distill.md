---
layout: distill
title: "Processing and acquisition traces in visual encoders: What does CLIP know about your camera?"
description: a lot, apparently
tags: distill formatting
giscus_comments: true
date: 2000-01-01
featured: true
categories: main

authors:
  - name: Ryan Ramos
    url: "https://ramos-ramos.github.io/"
    affiliations:
      name: The University of Osaka
    notation: "*"
  - name: Vladan Stojnić
    url: "https://stojnicv.xyz/"
    affiliations:
      name: VRG, Czech Technical University in Prague
    notation: "*"
  - name: Giorgos Kordopatis-Zilos
    url: "https://gkordo.github.io/"
    affiliations:
      name: VRG, Czech Technical University in Prague
  - name: Yuta Nakashima
    url: "https://www.n-yuta.jp/"
    affiliations:
      name: The University of Osaka
  - name: Noa Garcia
    url: "https://www.noagarciad.com/"
    affiliations:
      name: The University of Osaka

notations:
  - symbol: "*"
    description: Equal contribution.

bibliography: camera_bias.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction
  - name: Processing and acquisition traces can be recovered
    subsections:
      - name: Recovering processing labels
      - name: Recovering acquisition labels 
  - name: Traces can interfere with downstream tasks
    subsections:
      - name: Processing traces affect nearest neighbors
      - name: Acquisition traces distract in image retrieval
  - name: Discussion

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<span id="introduction" style="position: relative; top: -66px;"></span>

<!-- Visual encoders are everywhere because image representations are the starting point for many vision tasks. Because of this ubiquity, multiple works have probed the sensitivity of these encoders to distribution shifts such as corruptions or stylistic changes. In our paper [Processing and acquisition traces in visual encoders: What does CLIP know about your camera?](https://arxiv.org/abs/2508.10637), we show that visual encoders are also sensitive to **processing parameters** (e.g. JPEG compression) and **acquisition parameters** (e.g. camera model). This information can be recovered from image representations depending on the visual encoder, and this information can overshadow the image's semantic information and affect performance on downstream tasks. -->

It turns out that this whole time, some visual encoders are sensitive to **processing parameters** (e.g. JPEG compression) and **acquisition parameters** (e.g. camera model). This information can be recovered from the image representations produced by these visual encoders, which in turn can overshadow the image's semantic information and affect performance on downstream tasks. This blog post provides a brief overview of these findings, which are detailed further in our ICCV 2025 highlight paper [Processing and acquisition traces in visual encoders: What does CLIP know about your camera?](https://arxiv.org/abs/2508.10637).

<div class="l-body">
  <iframe src="{{ '/assets/plotly/camera_bias/sim_dist.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
<div class="figcaption">If you take an ImageNet image and calculate its cosine similarity with all the other images, you might get distributions that look like these. The similarities with images of the same class will skew higher than those with different classes, but whether or not the images have the same JPEG quality as the query image can also affect the similarities.</div>


## Processing and acquisition traces can be recovered

To show that vision encoders incorporate processing and acquisition traces into their embeddings (and determine which encoders do this the most), we train linear classifiers over embeddings extracted from a variety of vision encoders, with the encoders kept frozen. We categorize them as:

- **Supervised (SUP)**: Models trained in a supervised manner (e.g. with an image classification objective). We study several variants of ResNet<d-cite key="he2016deep"></d-cite>, ConvNeXt<d-cite key="liu2022convnet"></d-cite>, and ViT<d-cite key="dosovitskiy2020image"></d-cite>.
- **Self-supervised learning (SSL)**: Models trained in a self-supervised manner (e.g. contrastive learning). We study several variants of DINO<d-cite key="caron2021emerging"></d-cite>, DINOv2<d-cite key="oquab2024dinov2"></d-cite>, and MoCov3<d-cite key="chen2021empirical"></d-cite>.
- **Contrastive visual-language (CVL)**: Models trained for vision-language alignment. We study several variants of CLIP<d-cite key="radford2021learning"></d-cite>, OpenCLIP<d-cite key="cherti2023reproducible"></d-cite>, and SigLIP<d-cite key="zhai2023sigmoid"></d-cite><d-cite key="tschannen2025siglip"></d-cite>.

The idea is that if you can predict traces from a frozen model's embeddings at rates higher than random chance, then the model is definitely encoding relevant information. Across our experiments we tune learning rate and weight decay, but we use different datasets and processing steps depending on what we're trying to predict. These are covered in the next subsections.

### Recovering processing labels

To probe for processing traces, we can take a dataset like ImageNet<d-cite key="russakovsky15imagenet"></d-cite> or iNaturalist 2018<d-cite key="horn18inaturalist"></d-cite><d-cite key="inaturalist2018"></d-cite> and process them ourselves, creating our own image-label pairs. We choose the following parameters:

<div class="figcaption">The processing parameters we analyze.</div>

| parameter        | # classes | description                         |
| ---------------- | --------: | ----------------------------------- |
| JPEG compression | 6         | amount of JPEG compression          |
| sharpening       | 3         | amount of sharpening                |
| resizing         | 3         | amount of resizing                  |
| interpolation    | 4         | type of interpolation during resize |

For the specific classes we study and their implementation, feel free to check out the supplementary material of our paper.

In creating our pairs for a specific parameter, we seek to balance our data, so the specific class we process our image into is randomly sampled. For example, when we're creating our data for probing JPEG compression, there is $$ \frac{1}{6} $$ chance that the image is compressed with quality 95 and chroma-subsampling 4:2:0, a $$ \frac{1}{6} $$ chance that the quality is 95 and chroma-subsampling is 4:4:4, so on and so forth.

<style>
.original-container {
  display: grid;
  grid-template-columns: repeat(1, 80px);
  grid-auto-rows: 80px;                  
  gap: 5px;
}

.processed-container {
  display: grid;
  grid-auto-flow: column;
  grid-auto-columns: 80px;               
  gap: 5px;
}

.image-box {
  display: flex;
  flex-direction: column;
  align-items: center;
  font-size: 14px;
  text-align: center;
}

.image-box img {
  width: 100%;
  height: 80px;
  object-fit: cover;
  display: block;
}

.image-box span {
  margin-top: 4px;
}
</style>

<div class="wrapper" style="gap: 2em;">
  <div>
    <div class="original-container">
      <div class="image-box">
        <span>source</span>
        <img src="/assets/img/camera_bias/query.jpg">
      </div>
    </div>
  </div>

  <div>
    <div class="processed-container">
      <div class="image-box">
        <span>75, 4:2:0</span>
        <img src="/assets/img/camera_bias/quality=75_subsampling=2.jpg">
      </div>
      <div class="image-box">
        <span>75, 4:4:4</span>
        <img src="/assets/img/camera_bias/quality=75_subsampling=0.jpg">
      </div>
      <div class="image-box">
        <span>85, 4:2:0</span>
        <img src="/assets/img/camera_bias/quality=85_subsampling=2.jpg">
      </div>
      <div class="image-box">
        <span>85, 4:4:4</span>
        <img src="/assets/img/camera_bias/quality=85_subsampling=0.jpg">
      </div>
      <div class="image-box">
        <span>95, 4:2:0</span>
        <img src="/assets/img/camera_bias/quality=95_subsampling=2.jpg">
      </div>
      <div class="image-box">
        <span>95, 4:4:4</span>
        <img src="/assets/img/camera_bias/quality=95_subsampling=0.jpg">
      </div>
    </div>
  </div>
</div>
<div class="figcaption">An example of our probing data creation process using the JPEG compression parameter. Each image is processed into one of these classes at equal chance (in this example, the numbers refer to the quality and chroma-subsampling of the compression).</div>

<br>

The results of our experiments are shown below:

<div class="l-page">
  <iframe src="{{ '/assets/plotly/camera_bias/acc_processing_prediction.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
<div class="figcaption">Test accuracies on processing label prediction. Hover over each bar to see which visual encoder it belongs to. CVLs perform well above random performance, with supervised ConvNeXts also showing good performance. SSL models perform closer to random accuracy.</div>

First of all, it's clear that some visual encoder are performing well above random accuracy, meaning that processing traces are being stored in their embeddings. We observe this most strongly in CVLs, which can reach 80% test accuracy on JPEG compression (random chance is 16.67%, which is more than 4$$ \times $$ lower!). We also see that supervised ConvNeXts perform well, which is surprising given that the only information available during training are ImageNet labels. Lastly, we note that SSL models tend to perform the weakest. These observations are a commonly reoccuring point throughout this work.


### Recovering acquisition labels

To probe for acquisition traces, we need a dataset that comes with relevant annotations. Fortunately, this can be extracted from images' Exif metadata. Unfortunately, many readily available datasets don't provide this metadata. Thus, we use the Flickr API<d-footnote><a href="https://www.flickr.com/services/api/">https://www.flickr.com/services/api/</a></d-footnote> to search for safe-for-work, permissively licensed photos and extract their Exif metadata to create our image-label pairs.

Now, there’s a possibility that there are correlations between image semantics and acquisition labels. For example, if nighttime photos are commonly shot with higher ISO values, then instead of determining whether encoders are sensitive to the low-level features associated with different ISO values, we might simply end up determining which ones can distinguish between daytime and nighttime photos.

To avoid this, we have two fixes. The first is to limit the amount of photos a Flickr user contributes per month-year in our dataset (if wedding photographer Bob took 1,000 photos in one day with similar camera settings, we'd be in trouble). The second is to simply mask a large portion of the image (90%) to scrub out the semantic information. The first fix is incorporated into our dataset, while we leave the second as the recommend preprocessing procedure for future users.

Our final dataset, which we name FlickrExif, is available here <span style="color: red;">NOTE: FlickrExif is not up yet, ILIAS is placed as a dummy</span>:

<div class="l-page">
  <iframe
    src="https://huggingface.co/datasets/vrg-prague/ilias/embed/viewer/core_db/core_db"
    frameborder="0"
    width="100%"
    height="560px"
  ></iframe>
</div>

As for a quick overview of the acquisition parameters we study with FlickrExif:

<div class="figcaption">The acquisition parameters we analyze.</div>

| parameter                  | # classes | description                          |
| -------------------------- | --------: | ------------------------------------ |
| make                       | 9         | manufacturer of the camera           |
| model (all)                | 88        | specific camera model used           |
| model (smart)              | 12        | specific smartphone used             |
| model (smart vs non-smart) | 2         | whether camera is a smartphone       |
| exposure                   | 16        | amount of light captured by sensor   |
| aperture                   | 17        | size of the opening in the lens      |
| ISO speed                  | 16        | camera sensor's sensitivity to light |
| focal length               | 13        | distance from lens to sensor         |

With that established, our results are below:

<div class="l-page">
  <iframe src="{{ '/assets/plotly/camera_bias/acquisition_plots_ninety.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
<div class="figcaption">Test accuracies on acquisition label prediction. Hover over each bar to see which visual encoder it belongs to. Similar to the last experiment, we observe strong performance from CVLs and supervised ConvNeXts and near-random performance from SSL models.</div>

We observe a very similar pattern in these results. CVLs, with the exception of SigLIPs, perform very well, alongside supervised ConvNeXts. Meanwhile, SSL models seem to encode very little acquisition information.


## Traces can interfere with downstream tasks

Now that we’ve (quite clearly) established that some visual encoders do indeed encode processing and acquisition traces, we want to show why you should care. We show an example for each type of trace: kNN classification and image retrieval.

### Processing traces affect nearest neighbors

Consider four different types of training sets for a kNN classifier:

- **All-same (baseline)**: all train and test images are processed identically 
- **All-diff**: test images are processed differently from train images
- **Pos-same**: train and test images only have the same processing attributes if they share the same class, and are different otherwise
- **Neg-same**: train and test images only have the same processing attributes if they do not share the same class, and are identical otherwise
- **Uniform**: train images are processed one way while the test images are processed randomly in a uniform manner

We plot the results for each set-up below:

<div class="l-page">
  <iframe src="{{ '/assets/plotly/camera_bias/processing_influence.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
<div class="figcaption">Accuracies across the different set-ups. For some models, whether the query image's metadata aligns with the positives or negatives significantly impacts performance. For some, there is very little effect (try zooming in on the SSL models). ImageNet classification uses $ k=10 $ whie iNaturalist classification uses $ k=1 $.</div>

If you look at the CVLs, we can see that performance wildly differs depending on whether it's the positivies or the negatives that match the query's metadata. In the latter case, the metadata becomes the dominant factor in determining the nearest neighbors, essentially "distracting" the classifier and tanking its performance. Conversely, we see that processing has minimal effect in the case of SSL models, showcasing the lack of potentially distracting processing traces in their embeddings.

### Acquisition traces distract in image retrieval

Imagine an image retrieval scenario, where the target image was captured with a type of camera different from the one used to capture the query image. Now imagine two possible collections:

- **same**: one where the rest of the collection were also captured by a different camera
- **different**: one where the non-target collection images were captured with the same camera as the query

Similar to the previous subsection, we show that acquisition traces, in this case camera type, can also dominate over and distract from semantic information, affecting retrieval.

To do this, we collect a dataset we dub PairCams. We capture 730 pairs of photos of the *exact same subject* with nearly *identical shooting conditions* (e.g. angle, camera orientation, time of day, camera shooting mode), with the difference between camera type. We experiment with modern smartphones and older digital cameras. Despite the difference in camera, the image pairs contain nearly identical semantic content, meaning that it should be trivial to retrieve one using the other.

<style>
.wrapper {
  display: flex;
  gap: 5rem;       /* space between left and right grids */
  justify-content: center;
  flex-wrap: wrap; /* allows stacking on narrow screens */
}

.grid-container {
  display: grid;
  grid-template-columns: repeat(2, 150px); /* 2 columns */
  grid-auto-rows: 150px;                   /* 4 rows (automatically) */
  gap: 5px;
}

.grid-container img {
  width: 100%;
  height: 100%;
  object-fit: cover;   /* handle different image sizes */
  transition: opacity 0.3s;
  display: block;
}

.dimmed {
  opacity: 0.3;

.grid-title {
  text-align: center;     /* center the text above the grid */
  margin-bottom: 0.5rem;  /* space between title and grid */
  font-weight: bold;      /* optional, make it bold */
  font-size: 1rem;        /* adjust size as needed */
}
}
</style>

<div class="wrapper">
  <!-- Left grid -->
  <div>
    <div class="grid-title">modern smartphones</div>
    <div class="grid-container" id="left-grid">
      <img src="/assets/img/camera_bias/b_0.jpg" data-index="0">
      <img src="/assets/img/camera_bias/b_1.jpg" data-index="1">
      <img src="/assets/img/camera_bias/b_2.jpg" data-index="2">
      <img src="/assets/img/camera_bias/b_3.jpg" data-index="3">
      <img src="/assets/img/camera_bias/b_4.jpg" data-index="4">
      <img src="/assets/img/camera_bias/b_5.jpg" data-index="5">
      <img src="/assets/img/camera_bias/b_6.jpg" data-index="6">
      <img src="/assets/img/camera_bias/b_7.jpg" data-index="7">
    </div>
  </div>

  <!-- Right grid -->
  <div>
    <div class="grid-title">older digital cameras</div>
    <div class="grid-container" id="right-grid">
      <img src="/assets/img/camera_bias/a_0.jpg" data-index="0">
      <img src="/assets/img/camera_bias/a_1.jpg" data-index="1">
      <img src="/assets/img/camera_bias/a_2.jpg" data-index="2">
      <img src="/assets/img/camera_bias/a_3.jpg" data-index="3">
      <img src="/assets/img/camera_bias/a_4.jpg" data-index="4">
      <img src="/assets/img/camera_bias/a_5.jpg" data-index="5">
      <img src="/assets/img/camera_bias/a_6.jpg" data-index="6">
      <img src="/assets/img/camera_bias/a_7.jpg" data-index="7">
    </div>
  </div>
</div>
<div class="figcaption">Examples from PairCams. Each image is captured twice, once with a modern smartphone and once with an older digital camera. The whole dataset is available <a href="https://huggingface.co/datasets/ryanramos/PairCams">here</a>.</div>

<script>
function setupHoverSync(leftGrid, rightGrid) {
  function handleHover(e, sourceGrid, targetGrid) {
    if (e.target.tagName !== "IMG") return;
    const idx = e.target.dataset.index;

    sourceGrid.querySelectorAll("img").forEach(img => {
      if (img.dataset.index !== idx) img.classList.add("dimmed");
    });
    targetGrid.querySelectorAll("img").forEach(img => {
      if (img.dataset.index !== idx) img.classList.add("dimmed");
    });
  }

  function reset(e, sourceGrid, targetGrid) {
    sourceGrid.querySelectorAll("img").forEach(img => img.classList.remove("dimmed"));
    targetGrid.querySelectorAll("img").forEach(img => img.classList.remove("dimmed"));
  }

  leftGrid.addEventListener("mouseover", e => handleHover(e, leftGrid, rightGrid));
  rightGrid.addEventListener("mouseover", e => handleHover(e, rightGrid, leftGrid));
  leftGrid.addEventListener("mouseout", e => reset(e, leftGrid, rightGrid));
  rightGrid.addEventListener("mouseout", e => reset(e, rightGrid, leftGrid));
}

setupHoverSync(
  document.getElementById("left-grid"),
  document.getElementById("right-grid")
);
</script>

We go through ever visual encoder we've used so far to calculate the recall@$1$ for each collection:

<div class="l-page">
  <iframe src="{{ '/assets/plotly/camera_bias/ndidscatter.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
<div class="figcaption">Recall@$1$ on the *same* and *different* collections. Zoom into the square for a clearer look. Again, CVL models show the most sensitivity, with SSL models showing the least. A plot on $y=x$ implies completely robust performance.</div>

The $ y=x $ line shows where a perfectly robust visual encoder should lie. If a visual encoder is not prone to being distracted by acquisition parameters as it searches for the best semantic match, then performance should be equal in both settings. What we observe however is that CVLs again show an extreme sensitivity to acquisition parameters. A visual encoder with a near perfect recall@$1$ with the *different* collection can drop to 0.85 recall@$1$ in the *same* setting. Zooming into the black square shows another consistent findings: SSL models are among the most robust, staying much closer to the $y=x$ line than any other model.

## Discussion

The evidence that some visual encoders also encode processing and acquisition traces is strong, but we're still in the middle of figuring out the possibly *why's* behind all of this. It's a little difficult to test our theories given the use of private datasets for existing models, and the cost of training our own models from scratch. We also have yet to think of mitigation techniques.

This phenomenon we observe also carries implications. Firstly, the fact that metadata traces can overshadow semantic information raises concerns over the robustness and trustworthiness of our current algorithms. We wouldn't want a malicious agent messing with a model simply by playing with images' metadata, especially in critical domains like healthcare or autonomous systems. However, there is also the implication that this information being available through off-the-shelf models can help with digital forensics research or deepfake detection.
